{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第一步：理解核心区别**\n",
    "* 线性回归：预测一个具体的数值（例如：房价是 200 万，气温是 25.5 度）。\n",
    "* 分类问题：预测一个类别/标签（例如：这封邮件是垃圾邮件吗？这张图片是猫还是狗？）。\n",
    "\n",
    "在最简单的“二分类”问题中，结果只有两种：0 (No/Negative) 和 1 (Yes/Positive)。"
   ],
   "id": "ccf16d31e52b382a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第二步：你的 10 个样本数据集**\n",
    "假设你是一位老师，你想根据学生的**复习时长（小时）来预测他们能否通过考试**。\n",
    "* $x$ (特征)：复习时长。\n",
    "* $y$ (标签)：0 代表“挂科”，1 代表“通过”。\n",
    "\n",
    "这是一份包含 10 位学生的数据：\n",
    "| 学生 | 复习时长 (小时) | 通过考试 (y) |\n",
    "|:---:|:---:|:---:|\n",
    "| 1    | 0.5| 0（挂科）    |\n",
    "| 2    | 1.0| 0（挂科）    |\n",
    "| 3    | 1.5| 0（挂科）    |\n",
    "| 4    | 2.0| 0（挂科）    |\n",
    "| 5    | 2.5| 0（挂科）    |\n",
    "| 6    | 3.0| 1（通过）    |\n",
    "| 7    | 3.5| 1（通过）    |\n",
    "| 8    | 4.0| 1（通过）    |\n",
    "| 9    | 4.5| 1（通过）    |\n",
    "| 10   | 5.5| 1（通过）    |\n",
    "\n",
    "**直觉观察**：你会发现一个明显的**分界线**。大概在**3.0 小时**左右，复习时间小于这个的都挂了，大于这个的都过了。"
   ],
   "id": "61493eaaf53c5e30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第三步：为什么要抛弃线性回归？**\n",
    "如果你强行用线性回归去画一条直线 $y = wx + b$ 来拟合这些点，会发生什么？\n",
    "1. **数值越界**：对于复习 10 小时的学霸，直线算出来的结果可能是 $y = 2.5$。但结果只能是 0 或 1，但是 2.5 是什么意思？“通过了 2.5 次”吗？这没有意义。\n",
    "2. **含义不明**：我们想要的是一个概率，比如“有 80% 的几率通过”。\n",
    "\n",
    "**解决方案**：我们需要一个函数，把线性回归算出来的那个无限延伸的直线结果，“**压缩**”到 $(0, 1)$ 这个区间内。"
   ],
   "id": "744fc837f0a6c819"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第四步：引入神奇的“S型”函数 (Sigmoid)**\n",
    "这就是分类问题的核心武器：**Sigmoid 函数**。它的图像像一个被拉长的“S”。\n",
    "**数学公式**：\n",
    "\\begin{aligned}\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{aligned}\n",
    "* 这里的 $z$ 就是我们熟悉的线性回归公式：$z = wx + b$。\n",
    "* $e$ 是自然常数（约等于 2.718）。\n",
    "\n",
    "**通俗解释**：无论 $z$ 是多大多大的正数（比如 10000），$g(z)$ 都会无限接近 1。无论 $z$ 是多小的负数（比如 -10000），$g(z)$ 都会无限接近 0。如果 $z = 0$，那么 $g(z)$ 正好是 0.5。 <br>\n",
    "<br>\n",
    "**结合**：我们将线性回归套进这个壳子里，就变成了**逻辑回归**的预测公式：\n",
    "\\begin{aligned}\n",
    "h(x) = \\frac{1}{1 + e^{-(wx + b)}}\n",
    "\\end{aligned}"
   ],
   "id": "a8790120efbbc0f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第五步：回到例子进行预测**\n",
    "通过计算机（在这个教程中我们跳过复杂的“梯度下降”训练过程，直接给出训练好的最佳参数），我们找到了最适合这 10 个样本的参数：\n",
    "假设训练出的参数为：\n",
    "* 权重 $w = 3$\n",
    "* 偏置 $b = -8.4$（注：这是为了方便演示的近似值，目的是让分界线出现在 2.8 小时左右）\n",
    "\n",
    "现在的预测公式是：\n",
    "\\begin{aligned}\n",
    "h(x) = \\frac{1}{1 + e^{-(3x - 8.4)}}\n",
    "\\end{aligned}\n",
    "这个结果代表“**通过考试的概率**”。\n",
    "<br>\n",
    "<br>\n",
    "**让我们来算两个具体的例子**：<br>\n",
    "**例子 A：学生 4 (复习了 2.0 小时)**\n",
    "1. 先算线性部分：$z = 3 \\times 2.0 - 8.4 = 6 - 8.4 = -2.4$\n",
    "2. 放入 Sigmoid：$h(2.0) = \\frac{1}{1 + e^{-(-2.4)}} \\approx \\frac{1}{1 + 11.02} \\approx 0.08$\n",
    "3. 结论：概率为 0.08（即 8%）。因为 $0.08 < 0.5$，机器判定：0 (挂科)。<br>\n",
    "与真实数据对比：正确。\n",
    "\n",
    "**例子 B：学生 8 (复习了 4.0 小时)**\n",
    "1. 先算线性部分：$z = 3 \\times 4.0 - 8.4 = 12 - 8.4 = 3.6$\n",
    "2. 放入 Sigmoid：$h(4.0) = \\frac{1}{1 + e^{-(3.6)}} \\approx \\frac{1}{1 + 0.027} \\approx 0.97$\n",
    "3. 结论：概率为 0.97（即 97%）。因为 $0.97 > 0.5$，机器判定：1 (通过)。<br>\n",
    "与真实数据对比：正确。"
   ],
   "id": "2059ed0b77b34fe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第六步：决策边界 (Decision Boundary)**\n",
    "机器怎么决定是判 0 还是判 1？通常我们设定一个阈值 (Threshold)，通常是 0.5。\n",
    "* 如果 $h(x) \\geq 0.5$，判定为 1 (通过)。\n",
    "* 如果 $h(x) < 0.5$，判定为 0 (挂科)。\n",
    "\n",
    "**数学上的分界线在哪里**？当 $h(x) = 0.5$ 时，意味着 $z = 0$。\n",
    "\\begin{aligned}\n",
    "3x - 8.4 & = 0 \\\\\n",
    "3x & = 8.4 \\\\\n",
    "x & = 2.8\n",
    "\\end{aligned}\n",
    "这意味着，**2.8 小时**就是那条看不见的红线。\n",
    "* 复习多于 2.8 小时 -> 预测通过。\n",
    "* 复习少于 2.8 小时 -> 预测挂科。"
   ],
   "id": "6ba18f1667d3f818"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **第七步：损失函数 (Loss Function)**\n",
    "在线性回归中，我们用“均方误差”（$(y - \\hat{y})^2$）来衡量模型好坏。<br>\n",
    "但在分类问题中，这个方法不太好用了（因为它会导致数学图像弯弯曲曲，很难找到最低点）。<br>\n",
    "我们改用**对数损失 (Log Loss)**。<br>\n",
    "\n",
    "我们现在有了一堆样本$(x^{(i)},\\, y^{(i)}),\\; i = 1, \\ldots, m$ <br>\n",
    "目标： 找到一组参数 $w$ 和 $b$，让模型给这堆数据的“概率”最大 —— 这就是最大似然估计（Maximum Likelihood Estimation，MLE）。\n",
    "\n",
    "#### **1. 写出单个样本的“似然”**\n",
    "对于第 \\( i \\) 个样本：\n",
    "* 模型预测：$\\hat{p}^{(i)} = P(y^{(i)} = 1 \\mid x^{(i)}; w, b)$\n",
    "* 伯努利分布给出：\n",
    "$$P(y^{(i)} \\mid x^{(i)}; w, b) = (\\hat{p}^{(i)})^{y^{(i)}} (1 - \\hat{p}^{(i)})^{1 - y^{(i)}}$$\n",
    "> 这里的 $\\hat{p}^{(i)}$ 其实就是 $\\sigma(w^T x^{(i)} + b)$。\n",
    "\n",
    "#### **2. 写出所有样本的联合似然**\n",
    "假设每个样本是独立的（iid），则所有样本的\"联合概率\"（似然函数）是：\n",
    "\\begin{aligned}\n",
    "L(w, b) = \\prod_{i=1}^m P(y^{(i)} \\mid x^{(i)}; w, b) = \\prod_{i=1}^m (\\hat{p}^{(i)})^{y^{(i)}} (1 - \\hat{p}^{(i)})^{1 - y^{(i)}}\n",
    "\\end{aligned}\n",
    "\n",
    "**最大似然估计**：<br>\n",
    "我们想要找参数，使得这个乘积最大：\n",
    "\\begin{aligned}\n",
    "\\max_{w, b} L(w, b)\n",
    "\\end{aligned}\n",
    "\n",
    "#### **3. 取对数（变乘为和）**\n",
    "直接优化这个乘积很麻烦，所以通常取对数，得到**对数似然（log-likelihood）**：\n",
    "\\begin{aligned}\n",
    "\\ell(w, b) = \\log L(w, b) = \\sum_{i=1}^m \\left[ y^{(i)} \\log \\hat{p}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{p}^{(i)}) \\right]\n",
    "\\end{aligned}\n",
    "* 取 log 之后，乘积变求和；\n",
    "* 因为 log 是单调递增的，最大化 $L(w, b)$ 等价于最大化 $\\ell(w, b)$。\n",
    "\n",
    "#### **4. 换成\"最小化损失\"的形式**\n",
    "机器学习 / 深度学习中，我们习惯写成\"**损失函数越小越好**\"。<br>\n",
    "于是：\n",
    "* 最大化对数似然：$\\max \\ell(w, b)$\n",
    "* 等价于**最小化负对数似然 (negative log-likelihood)**：\n",
    "\n",
    "\\begin{aligned}\n",
    "J(w, b) = -\\ell(w, b) = -\\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\hat{p}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{p}^{(i)}) \\right]\n",
    "\\end{aligned}\n",
    "\n",
    "再除以样本数 $m$，得到平均损失：\n",
    "\\begin{aligned}\n",
    "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\hat{p}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{p}^{(i)}) \\right]\n",
    "\\end{aligned}\n",
    "\n",
    "**这就是我们常说的**：<br>\n",
    "> 逻辑回归的**交叉熵损失 (log loss)**。\n",
    "\n",
    "对应到单个样本上：\n",
    "\\begin{aligned}\n",
    "L(\\hat{p}, y) = -(y \\log \\hat{p} + (1 - y) \\log (1 - \\hat{p}))\n",
    "\\end{aligned}"
   ],
   "id": "5a76acb9843a97b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
